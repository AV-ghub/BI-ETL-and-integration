### **Краткий рекап по SSIS из предоставленных глав (1-5)**

**Основная идея:** SSIS — это enterprise-grade ETL-инструмент, "кровеносная система" BI-решений. Он отделяет **Control Flow** (логика выполнения задач) от **Data Flow** (движение и трансформация данных).

---

### **Блок 1: Архитектура и ключевые концепции (Глава 1, 4)**
*(Это основа, вопросы по этому блоку задают практически всегда)*

**1. Control Flow vs. Data Flow (Чем отличаются?)**
*   **Control Flow:** Оркестрирует *задачи* (Tasks) и *контейнеры* (Containers). Определяет *последовательность* и *условия* выполнения. Не работает с данными напрямую. Элементы: Execute SQL Task, File System Task, For Loop Container и т.д.
*   **Data Flow:** Движение и преобразование *данных*. Выполняется *внутри* специальной задачи Control Flow — **Data Flow Task**. Работает с буферами в памяти для скорости. Элементы: Source, Transformation (Lookup, Merge, Derived Column), Destination.

**2. Основные компоненты Data Flow:**
*   **Source:** Извлекает данные (OLE DB, Flat File, Excel).
*   **Transformation:** Меняет данные в буфере (сортировка, объединение, очистка).
*   **Destination:** Загружает данные.

**3. Проект и пакет (.dtproj vs .dtsx):**
*   **Проект (Project):** Контейнер для пакетов. В SSIS 2012+ появилась **проектная модель развертывания** (Project Deployment Model) с каталогом SSISDB, параметрами и окружениями.
*   **Пакет (Package):** Исполняемая единица (.dtsx файл). Содержит Control Flow, Data Flow, переменные, конфигурации.

**4. Основные Connection Managers (Менеджеры соединений):**
*   **OLE DB / ADO.NET:** Для подключения к SQL Server и другим БД. На собеседовании могут спросить про разницу (OLE DB — родной, обычно быстрее; ADO.NET — managed-провайдер, удобен для Script Task).
*   **Flat File:** Для текстовых файлов (с разделителями, фиксированной ширины). Важно знать настройки кодовой страницы (Code Page), заголовков, типов данных в Advanced-редакторе.
*   **Cache Connection Manager:** Специфичен для SSIS. Кеширует набор данных в памяти для повторного использования (например, несколькими Lookup-преобразованиями). Ускоряет работу, уменьшает нагрузку на источник.

---

### **Блок 2: Среда разработки и первый пакет (Глава 2, 3)**
*(Вопросы по инструментам и базовой практике)*

**1. BIDS / SSDT (Среда разработки):**
*   Проекты: Integration Services, Analysis Services, Reporting Services.
*   **Ключевые улучшения в SSIS 2012:** Отмена/повтор действий (Undo/Redo), помощники Source/Destination Assistant, улучшенный редактор сопоставления колонок (Resolve References).

**2. Лучшие практики для собеседования:**
*   **Именование:** Использовать префиксы (например, `DFT_` для Data Flow Task, `SQL_` для Execute SQL Task, `FF_SRC_` для Flat File Source). Показывает внимание к деталям.
*   **Аннотации (Annotations):** Добавлять в пакет описание, дату изменения, цель, список ключевых параметров. Для читаемости и поддержки кода.
*   **Защита чувствительных данных (ProtectionLevel):** Не хранить пароли в пакете. Использовать `DontSaveSensitive` + параметры/конфигурации или `EncryptSensitiveWithUserKey` (только для разработки).
*   **Валидация:** Свойство `DelayValidation` на пакете/задачах. `True` — чтобы пакет открывался без подключения к источникам (разработка). `False` (по умолчанию) — для раннего выявления ошибок.

---

### **Блок 3: Основные задачи Control Flow (Глава 5)**
*(Что вы будете использовать постоянно)*

**1. Execute SQL Task:**
*   **Для чего:** Выполнение T-SQL, DDL, вызов хранимок.
*   **Важные настройки:** `Connection`, `SQLStatement`, `ResultSet` (если ждете результат).
*   **Частый кейс:** Очистка таблицы-приемника (`TRUNCATE`) перед загрузкой в Data Flow.

**2. Data Flow Task:**
*   **Сердце ETL.** Самая ресурсоемкая задача. Обязательно к глубокому изучению.

**3. Precedence Constraints (Ограничения следования):**
*   Определяют поток выполнения между задачами.
*   **Условия:** `Success`, `Failure`, `Completion`.
*   **Выражения (Expression):** Можно добавить логику (например, запускать следующую задачу, только если `@[User::FileCount] > 0`).

---

### **Практические примеры и вопросы с собеседований (из вашего текста)**

**Вопрос:** *Опишите простейший ETL-пакет для загрузки данных из CSV в SQL Server.*
**Ваш ответ (структура):**
1.  **Control Flow:**
    *   Execute SQL Task (`SQL_TruncateStaging`) — очистка staging-таблицы.
    *   Data Flow Task (`DFT_LoadFromCSV`) — загрузка данных.
2.  **Data Flow внутри DFT_LoadFromCSV:**
    *   **Flat File Source** -> Настроен Connection Manager к CSV (указан делимитер, кодировка, типы данных).
    *   **Преобразования (опционально):** Derived Column (приведение данных), Data Conversion (смена типа).
    *   **OLE DB Destination** -> Указана таблица в SQL, настроено сопоставление колонок (Mapping). Используется режим **Fast Load** для bulk-вставки.

**Вопрос:** *В чем проблема сортировки (Sort Transformation) больших объемов данных в Data Flow?*
**Ваш ответ:** Sort — **блокирующее** (blocking) преобразование. Оно требует загрузки *всех* данных в память перед сортировкой. Для больших наборов данных это может вызвать нехватку памяти и свопинг на диск, что резко снижает производительность. **Альтернатива:** Загрузить данные в staging-таблицу и использовать индексированные сортировки средствами СУБД.

**Вопрос:** *Как соединить данные из двух разных источников (например, двух CSV-файлов) в Data Flow?*
**Ваш ответ:** Нужно использовать **Merge Join Transformation**. Но для него **входные данные должны быть отсортированы** по ключу соединения. Поэтому потоки сначала проходят через **Sort Transformation**. После сортировки Merge Join выполняет JOIN (Inner, Left, Full Outer).

---
